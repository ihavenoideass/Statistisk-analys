---
title: "Inlamning 4"
author: "Umut Arslan"
date: "2023-03-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message=FALSE}
library(reticulate)
library(keras)
library(tensorflow)
library(caTools)
library(e1071)
library(kernlab)
library(tidyverse)
library(e1071)
library(caret)
library(ggplot2)

```

## Fashion data
We will start by loading the fashion data onto r and convert it to a data frame. Converting the lable column to a factor, this step is important because the SVM model requiers the labels to be categorical variabels, this is my y variable.

We will then standardize the features, x variabels, we standardize to ensure tha all of the features are on the same scale.

We will train the fashion data with 3 diffrent methods, PCA + KSVM, NN and CNN.

```{r, echo = FALSE}



# Laddar in data
load("fashion_data.RData")
data <- data.frame(data)

#Gör om label som faktor
data <- data %>% mutate(label = as.factor(label))

#Mina variabler X, pixels
train_features <- data[, -1]

#Min variabel Y, lable
train_labels <- (data[, 1])

#train_features <- scale(train_features)
train_features <- scale(train_features)

#X_train <- reshape(train_features, c(kk[1], 28, 28, 1))

#Y_train <- to_categorical(train_labels - 1, num_classes = 10)

#Gör om tränings featuresen till 28x28 matriser
#train_features <- array(train_features, #dim=c(nrow(train_features), 28, 28))

# "plattar" till mina features genom en matris
#train_features_flattened <- matrix(train_features, #nrow=nrow(train_features))

#train_features <- unlist(train_features)


```
## PCA

We will use PCA to reduce the dimensionality from 784 dimensions to n_components. The n_components is determined by the cumulative explained variance ratio against the numbers of components. I chose to use the number of components needed to explain ATLEAST 85% om the total variance. After we have calculated the n_components, we will apply the n_components from the PCA model into my KSVM model, kernal support vector machine.

I have plotted the cumulative explained variance ratio against the numbers of components down below, you could argue that after the first 8 components, the difference in the explained variance is minuscule, so you could probably chose 65-70% as the threshold to reduce alot of dimensions. The risk increases of overfitting the training-model when you are choosing more PCA-components. In other words, i have taken a higher risk of overfitting the model when i chose 85%, because my n_components = 79 PCA-components.
```{r, echo = FALSE}


# Apply PCA to reduce the dimensionality to 2
pca <- prcomp(train_features, center = TRUE, scale. = TRUE)

#Sätter att vi vill ha antalet komponenter som förklarar minst 85% av variansen
cumulative_variance <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
n_components <- min(which(cumsum(pca$sdev^2 / sum(pca$sdev^2)) >= 0.85))

plot(cumulative_variance, type = "b", xlab = "Number of Components", ylab = "Cumulative Explained Variance Ratio")

#str(train_labels)
train_pca <- predict(pca, train_features)
#train_pca <- (predict(pca, train_features)[, 1:n_components])


#y_pred <- predict(svm_model, train_features[, 1:n_components])



#accuracy <- sum(y_pred == train_labels) / length(train_labels)

#confusion_matrix <- table(Predicted = y_pred, True = train_labels)


#Sätter att vi vill ha antalet komponenter som förklarar minst 85% av variansen
#n_components <- min(which(cumsum(pca$sdev^2 / sum(pca$sdev^2)) >= 0.85))

#Väljer top n_PCA komponenter (eigenvectors)
#top_k_eigenvectors <- pca$rotation[, 1:n_components]
#dim(top_k_eigenvectors)
#Projecerar data till ett nytt feature space
#projected_features <- train_features %*% top_k_eigenvectors


```



## SVM

There are two ways to compile an SVM model, u can use the package e1071, svm(x, y, type, kernel, c, gamma) or u can use another package, kernlab, ksvm(x, y, type, kernel, kpar, and c )

* x = the X variabel, förklarinsgvariabel 
* Y = the y variabel, Responsvariabel
* type = what type of calculation you want to do, classification or regression. You can see more if u type, i am using C-svc ?ksvm
* kernel computes the inner product in the feature space between tow vector arguments. rbfdot, polydot, vanilladot, tanhdot etcetra is some of the kernal functions inside of the ksvm model

*kpar is the list of hyperparameters, kernel parameters. See in ?ksvm for more info, we only used sigma in my test. Sigma controls the width of the distribution, smaller value equals to sharper peak and larger sigma gives a broader peak.

* C is cost of constraints violationviolation,controls the trade-off between achieving a low training error and a low testing error.

We will use my 79 PCA-components combined with my training labels to train the KSVM model. I also used 
```{r, echo = FALSE}

# Train an SVM model on the projected training data

sampled_rows <- sample(nrow(pca$x), 1000)
sampled_rows2 <- sample(nrow(pca$x), 5000)
sampled_rows3 <- sample(nrow(pca$x), 10000)


## Using Mini-batch
systime_1000 <- system.time({
  ksvm_sample <- ksvm(pca$x[sampled_rows, 1:n_components], train_labels[sampled_rows], type = "C-svc", kernel = "rbfdot", C = 0.1, kpar = list(sigma = 1))
})

systime_5000 <- system.time({
  ksvm_sample2 <- ksvm(pca$x[sampled_rows2, 1:n_components], train_labels[sampled_rows], type = "C-svc", kernel = "rbfdot", C = 1, kpar = list(sigma = 1))
})


systime_10000 <- system.time({
  ksvm_sample3 <- ksvm(pca$x[sampled_rows3, 1:n_components], train_labels[sampled_rows], type = "C-svc", kernel = "rbfdot", C = 0.1, kpar = list(sigma = 1))
})




systime_30000 <- system.time({
  ksvm_model2 <- ksvm(pca$x[, 1:n_components], train_labels, type = "C-svc", kernel = "rbfdot", C = 1, kpar = list(sigma = 1))

})

time_df <- data.frame(
  "1000" = systime_1000[3],
  "5000" = systime_5000[3],
  "10000" = systime_10000[3],
  "30000" = systime_30000[3]
)

time_df
```
Above we can see the time it takes to compile the model based on how many vectors we have, the time is in seconds. All of the models above got an training error = 0, when using mini-batches, there are some backdraw of using mini-batch instead of using the whole dataset, the stochastic nature of mini-batching may result in noisy gradient estimates and the model can fluctuate quite alot. You will save alot of time tho. We could also have gotten training error = 0 becaus of overfitting, its quite hard to know if we dont test the trainingmodel with a testdata.


## Neural Network

We have different layers in the model.

* layer_flatten: flattens the image data, in my case i have flattend it to 28x28, 1-d vector.

*layer_dense: this is an hidden layer, we have two hidden layers, a "relu" and a "softmax" activation layer.

After determining the layers, we compile the model by sdpecifiying the loss, optimizer and metrics function

* loss: how well the neural network is able to predict the correct output
* optimizer: updating the weights of the neural network
* metrics: how  you want to evaluate the model, i chose accuracy.

lastly we fit the model with the training features and the training lables. we need to transform the lables using to_categorical, the lables were factorial before because of the SVM model.

* epochs: How many iterations
* batch_size: The number of sample per iteration


I will use relu activation = 64, i tried 16, 32, 64 and 128. For me the plot "fitted" the best whitout looking too overfitted.
epochs = 20, because it had diminishing return after that
batch_size = 32, had the most "regression" like slope, while 64 and 128 had more curvature.

This model is giving us an 4% training error, pretty ok.

```{r, echo = FALSE}
#Need to do this to be able to run the NN model
train_labels2 <- to_categorical(train_labels)


nn_model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(28, 28, 1)) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")

nn_model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

hist_nn_ep20_b_32_64 <- nn_model %>% fit(
  train_features, train_labels2,
  epochs = 20,
  batch_size = 32
)

nn_error <- 1- max(hist_nn_ep20_b_32_64$metrics$accuracy)






plot(hist_nn_ep20_b_32_64)

##Densitylayer_64
#plot(hist_nn_ep15_b_32)
#plot(hist_nn_ep30_b_32)

#plot(hist_nn_ep15_b_64)
#plot(hist_nn_ep30_b_64)
#plot(hist_nn_ep15_b_128)
#plot(hist_nn_ep30_b_128)


##Densitylayer_32
#plot(hist_nn_ep15_b_32_32)
#plot(hist_nn_ep30_b_32_32)
#plot(hist_nn_ep15_b_64_32)
#plot(hist_nn_ep30_b_64_32)
#plot(hist_nn_ep15_b_128_32)
#plot(hist_nn_ep30_b_128_32)

#save(nn_model, file = "nn_model.RData")




```




## Convolution Neural Network

Here we use the same data as the NN-model, but we also reshape our features to a 4D tensor, where the first dimension is the samples, second is the height of the pixels, third is the width of the pixels and last one is t he channels. We also use a 2D convolutional layer instead of a 1D layer, like NN does. 

* layer_conv_2d : 2D convolutional layer with filter, kernel size, activation output reLU.

* layer_max_pooling_2d: includes a pool_size, which reduces spatial size of the output from the convolutional layer

* layer_flatten: flattens the output of the max pooling layer to a 1D vector so it can be passed to the layer_dense

* layer_dense: same as on NN, hidden layers, reLU and softmax.


after the layers CNN has the same compilecode as NN

* loss: how well the neural network is able to predict the correct output
* optimizer: updating the weights of the neural network
* metrics: how  you want to evaluate the model, i chose accuracy.

lastly, like NN we have the fit of the model which consits of the same features.

* epochs: How many iterations
* batch_size: The number of sample per iteration


I will mirror the NN-model, reLU = 64, epochs = 20 and batch_size = 32



ps, CNN has all the hyperparameters as NN plus more, like in the conv_2d layer and max_pooling layer.

We got an much lower training error with this model compared to NN, 0.3%. I also plotted the epoches down below.
```{r, echo = FALSE}

train_features_reshaped <- array_reshape(train_features, c(dim(train_features)[1], 28, 28, 1))

cnn_model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")

cnn_model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)


hist_cnn_ep20_b_32_64 <- cnn_model %>% fit(
  train_features_reshaped, train_labels2,
  epochs = 20,
  batch_size = 32
)

cnn_error <- 1- max(hist_cnn_ep20_b_32_64$metrics$accuracy)

plot(hist_cnn_ep20_b_32_64)

cnn_error

```



## Summary

SVM + PCA took the longest time if i used all the datapoints, CNN came second and the fastet one was NN (if you ignore the mini-batches of SVM + PCA)

But if we look at the training error, SVM + PCA got an perfect score, CNN came second and NN came last.

Depending on what you want to do and how you tune the hyperparameters, your mileage may vary.